import os
import json
import yaml
import shutil

from pathlib import Path
from snakemake.utils import validate


include: "rules/common.smk"


configfile: os.path.join(workflow.basedir, "../config/config.yaml")


envvars:
    "TMPDIR",
    "SNAKEMAKE_PROFILE",


validate(
    config,
    str(workflow.main_snakefile).replace(
        "workflow/Snakefile", "config/config.schema.yaml"
    ),
)

SHARDS = make_shard_names(config["nshards"])


onstart:
    with open("config_used.yaml", "w") as outfile:
        yaml.dump(config, outfile)

    if not os.path.exists("logs"):
        os.makedirs("logs")


localrules:
    all,


"""
we have to fix some inputs to downstream applications if we are running all together:
we do this for the assembly because that config entry is referenced many times in the binning and annotation workflows
for entries like R1/R2, its eaiest to set those in the conditional block below, as those are only used by a single rule within a given workflow
"""

if config["stage"] == "all":
    config["assembly"] = f"megahit_{config['sample']}.assembly.fasta"

config["readdirs"] = get_readdirs()


"""
similarly, we have to mock the downsample parameters if we aren't running with stage=downsample
"""
if config["stage"] != "downsample":
    config["depths"] = [0]
    config["reps"] = [0]
if config["stage"] != "qc":
    config["manifest"] = None
    config["sortmerna_report"] = None


if not is_paired():
    if not config["stage"] in ["preprocess", "biobakery"]:
        raise ValueError(
            "Single-end analysis has only been implemented for the preprocess and biobakery modules"
        )


#
# Preprocess Module
#
module preprocess:
    snakefile:
        "rules/preprocess.smk"
    config:
        config


use rule * from preprocess as preprocess_*


#
# Biobakery Module
#
module biobakery:
    snakefile:
        "rules/biobakery.smk"
    config:
        config
    skip_validation:
        True


use rule * from biobakery as biobakery_*


#
# qc Module
#
module qc:
    snakefile:
        "rules/qc.smk"
    config:
        config
    skip_validation:
        True


use rule * from qc as qc_*


#
# Kraken Module
#
module kraken:
    snakefile:
        "rules/kraken.smk"
    config:
        config
    skip_validation:
        True


use rule * from kraken as kraken_*


#
# Utils Module
#
module utils:
    snakefile:
        "rules/utils.smk"
    config:
        config


#
# Assembly Module
#
if is_paired():

    module assembly:
        snakefile:
            "rules/assembly.smk"
        config:
            config
        skip_validation:
            True

    use rule * from assembly as assembly_*

    #
    # RGI Module
    #
    module rgi:
        snakefile:
            "rules/RGI.smk"
        config:
            config

    use rule * from rgi as rgi_*

    #
    # Binning Module
    #
    module binning:
        snakefile:
            "rules/binning.smk"
        config:
            config

    use rule * from binning as binning_*

    #
    # annotation Module
    #
    if config["stage"] in ["annotate", "all"]:

        module annotate:
            snakefile:
                "rules/annotate.smk"
            config:
                config
            skip_validation:
                True

        use rule * from annotate as annotate_*

    #
    # hla Module
    #
    module hla:
        snakefile:
            "rules/HLA.smk"
        config:
            config
        skip_validation:
            True

    use rule * from hla as hla_*

    #
    # downsample Module
    #
    module downsample:
        snakefile:
            "rules/downsample.smk"
        config:
            config
        skip_validation:
            True

    use rule * from downsample as downsample_*


if config["stage"] == "classify_bins":

    module classify_bins:
        snakefile:
            "rules/classify_bins.smk"
        config:
            config
        skip_validation:
            True

    use rule * from classify_bins as classify_bins_*


"""
Notes:
put all the logic here to connect pipeline stages (eg setting biobakery to using preprocessed results rather than config's R1 and R2
because the concat rule exists in both the preprocessing and kraken
snakefiles, we override the ins and outs of one of them.  Since the new
outputs aren't in rule all, this clips the rule from the dag

The wildcard_constraints below ensures that the sample wildcard never ends with _shard001
needing this probably means I could specify file names more cleanly :/
"""

if config["stage"] == "all":
    if config["lib_layout"] != "paired":
        raise ValueError(
            f"lib_layout must be 'paired' to run 'all' stages of the pipeline; please run modules individually (eg --config stage=preprocess)"
        )

    use rule cat_pair from biobakery as biobakery_cat_pair with:
        input:
            R1="hostdepleted/{sample}_R1.fastq.gz",
            R2="hostdepleted/{sample}_R2.fastq.gz",

    use rule RGI from rgi as rgi_RGI with:
        input:
            R1="hostdepleted/{sample}_R1.fastq.gz",
            R2="hostdepleted/{sample}_R2.fastq.gz",
            db=config["CARD_db_json"],

    use rule OptiType_subset_fastq from hla as hla_OptiType_subset_fastq with:
        input:
            R1="host/{sample}_all_host_reads_R1.fastq.gz",
            R2="host/{sample}_all_host_reads_R2.fastq.gz",
            alleles=config["optityper_hla_dna"],

    use rule megahit from assembly as assembly_megahit with:
        input:
            R1=["hostdepleted/{sample}_R1.fastq.gz"],
            R2=["hostdepleted/{sample}_R2.fastq.gz"],

    use rule concat_lanes_fix_names from kraken as kraken_concat_lanes_fix_names with:
        input:
            fq=touch("input_F"),
        output:
            fq=temp(touch("output_F")),

    use rule kraken_standard_run from kraken as kraken_kraken_standard_run with:
        input:
            R1="trimmed/{sample}_shard{shard}_trim_R1.fastq.gz",
            R2="trimmed/{sample}_shard{shard}_trim_R2.fastq.gz",
            db=config["kraken2_db"],
        output:
            out="kraken2/{sample}_shard{shard}_kraken2.out",
            unclass_R1="kraken2/{sample}_shard{shard}_kraken2_unclassified_1.fastq",
            unclass_R2="kraken2/{sample}_shard{shard}_kraken2_unclassified_2.fastq",
            report=temp("kraken2/{sample}_shard{shard}_kraken2.report"),
        log:
            e="logs/kraken_{sample}_shard{shard}.e",

    use rule compress_kraken_unclassified from kraken as kraken_compress_kraken_unclassified with:
        input:
            R1=[
                f"kraken2/{{sample}}_shard{shard}_kraken2_unclassified_1.fastq"
                for shard in SHARDS
            ],
            R2=[
                f"kraken2/{{sample}}_shard{shard}_kraken2_unclassified_2.fastq"
                for shard in SHARDS
            ],

    use rule kraken_merge_shards from kraken as kraken_kraken_merge_shards with:
        input:
            #reports = rules.kraken_kraken_standard_run.output.report,
            reports=[
                f"kraken2/{{sample}}_shard{shard}_kraken2.report" for shard in SHARDS
            ],
        output:
            out="kraken2/{sample}_kraken2.report",
        wildcard_constraints:
            sample=".+(?<!shard\d\d\d)",


# in "all" mode, we do NOT HLA type as we regularly lack sufficient host depth
if config["stage"] == "all":
    outputs = [
        rules.preprocess_all.input,
        rules.biobakery_all.input,
        rules.assembly_all.input,
        rules.kraken_all.input,
        rules.rgi_all.input,
        rules.binning_all.input,
        rules.annotate_all.input,
        # rules.hla_all.input,
    ]
else:
    outputs = getattr(rules, f"{config['stage']}_all").input


rule all:
    input:
        outputs,
    default_target: True
