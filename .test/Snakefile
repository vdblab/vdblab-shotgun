import os
import json
import yaml
import shutil

from pathlib import Path


# is a string so we dont have weird issues where ints get cast to floats, adding a ".0"
total_depths = ["100000", "10000000"]  # config["total_depths"]
reps = 1  # config["reps"]
simtypes = ["equalreads", "equalcoverage"]
output_reads = expand(
    "{rep}_depth{depth}_{simtype}_R{readdir}.fastq.gz",
    rep=reps,
    depth=total_depths,
    readdir=[1, 2],
    simtype=simtypes,
)
output_stats = expand(
    "{rep}_depth{depth}_{simtype}.statsfastq",
    rep=reps,
    depth=total_depths,
    simtype=simtypes,
)


localrules:
    all,
    make_input_table,


all_results = [
    output_reads,
    output_stats,
]


rule all:
    input:
        all_results,
        expand(
            "clean_{rep}_depth{depth}_{simtype}.done",
            rep=reps,
            depth=total_depths,
            simtype=simtypes,
        ),


rule get_references:
    output:
        mockdir=directory("mock"),
    shell:
        """
    set -eoux pipefail
    # wget complains if this is present
    if [ -f D6331.refseq.zip ]
    then
    echo "refs present"
    else
        wget -N https://s3.amazonaws.com/zymo-files/BioPool/D6331.refseq.zip
        unzip D6331.refseq.zip
    fi
    # snakemake doesn't create outputs labeled as directories
    mkdir -p {output.mockdir}

    # add some of the human genome chr21 to our reference pool. Since chr21 is
    # still pretty large and we want tests to run fast, here we subset to the first 24000
    # lines of the 644155  original lines.
    # the temp file is so we don't get curl  errors from piping the result
    curl -s  "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nucleotide&id=NC_060945.1&rettype=fasta&retmode=txt" > chr21_temp
    head -n 25000 chr21_temp > D6331.refseq/genomes/t2t_chr21.fasta
    rm chr21_temp

    # borrowed some logic from https://www.biostars.org/p/294920/ to smoosh together sequences
    # otherwise, art with generate nreads PER Sequence, not per genome
    for fasta in D6331.refseq/genomes/*fasta
    do
        name=$(basename $fasta | sed "s|.fasta||g")
        cat $fasta |  grep -v "^>" | awk  -v header=">${{name}}_concatenated" 'BEGIN {{ ORS=""; print header "\\n" }} {{ print }}'  > {output.mockdir}/${{name}}.fasta
    done

    """


rule make_input_table:
    """ this could be modified to incorporate different depth schemes per bug,
    but for a first pass we do even depths
    """
    input:
        mockdir="mock",
    output:
        outtable="sim_tables/{rep}_depth{depth}.tab",
    run:
        import glob
        import math

        mocks = glob.glob(input.mockdir + "/*.fasta")
        depth = int(wildcards.depth)
        nmocks = len(mocks)
        each_non_spike = math.floor(depth / nmocks)
        with open(output.outtable, "w") as outf:
            for mock in mocks:
                if each_non_spike != 0:
                    outf.write(f"{mock}\t{each_non_spike}\n")

def reads_or_coverage(wildcards):
    # this is lazy lazy lazy
    if int(wildcards.depth) > 1000000:
        fcov = 5
    else:
        fcov = 0.5
    reads_or_coverage = f"--rcount $nreads" if wildcards.simtype == "equalreads" else f"--fcov {fcov}"
    return reads_or_coverage

rule simulate:
    input:
        mockdir="mock",
        intable="sim_tables/{rep}_depth{depth}.tab",
    output:
        R1="{rep}_depth{depth}_{simtype}_R1.fastq.gz",
        R2="{rep}_depth{depth}_{simtype}_R2.fastq.gz",
        intermediate_trigger=touch(
            "{rep}_depth{depth}_{simtype}_individual_reference_fastq_present"
        ),
    params:
        reads_or_coverage=lambda wildcards: reads_or_coverage(wildcards),
        tmppre=lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_{wildcards.simtype}",
    container:
        "docker://ghcr.io/vdblab/art:2016.06.05"
    threads: 4
    shell:
        """
    set -euxo pipefail

    # look a loop! could use parallel but this is a one-off for generating test data.
    cat {input.intable} | while read ref nreads
    do
        refbase=$(basename $ref)
        art_illumina --seqSys HS25 --in $ref  --paired --len 150 {params.reads_or_coverage} --mflen 500 --sdev 10 --out {params.tmppre}_${{refbase}}_R --rndSeed {wildcards.rep}
    done

    ls {params.tmppre}*
    cat {params.tmppre}*R1.fq | pigz --best > {output.R1}
    cat {params.tmppre}*R2.fq | pigz --best > {output.R2}
    """


rule stats:
    input:
        R1="{rep}_depth{depth}_{simtype}_R1.fastq.gz",
        R2="{rep}_depth{depth}_{simtype}_R2.fastq.gz",
        intermediate_trigger="{rep}_depth{depth}_{simtype}_individual_reference_fastq_present",
    output:
        stats="{rep}_depth{depth}_{simtype}.statsfastq",
    params:
        tmppre=lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_{wildcards.simtype}",
    container:
        "docker://ghcr.io/vdblab/seqkit:2.3.1"
    threads: 8
    shell:
        """
    # gather all the per-reference fastq stats
    seqkit stats {params.tmppre}*.fq > {output.stats}
    # gather the final stats
    seqkit stats {input.R1} {input.R2} | tail -n+2  >> {output.stats}
    """


rule cleanup:
    input:
        "{rep}_depth{depth}_{simtype}_individual_reference_fastq_present",
        "{rep}_depth{depth}_{simtype}.statsfastq",
    output:
        touch("clean_{rep}_depth{depth}_{simtype}.done"),
    params:
        tmppre=lambda wildcards: f"tmp_{wildcards.rep}_depth{wildcards.depth}_{wildcards.simtype}",
    shell:
        """
        rm {params.tmppre}*fq
        rm {params.tmppre}*aln
        """
